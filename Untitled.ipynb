{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=156)\n",
    "\n",
    "# subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "\n",
    "# subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier의 예측 정확도는 0.420\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier 객체 생성\n",
    "dt_clf = DecisionTreeClassifier(max_depth = 200)\n",
    "# DecisionTreeClassifier을 이용하여 학습/예측/평가 수행\n",
    "dt_clf.fit(X_train_cnt_vect, y_train)\n",
    "dt_pred = dt_clf.predict(X_test_cnt_vect)\n",
    "print('DecisionTreeClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,dt_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier의 예측 정확도는 0.612\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier 객체 생성\n",
    "rf_clf = RandomForestClassifier(random_state = 200, n_estimators=200)\n",
    "# RandomForestClassifier을 이용하여 학습/예측/평가 수행\n",
    "rf_clf.fit(X_train_cnt_vect, y_train)\n",
    "rf_pred = rf_clf.predict(X_test_cnt_vect)\n",
    "print('RandomForestClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,rf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm의 예측 정확도는 0.097\n"
     ]
    }
   ],
   "source": [
    "# svm 객체 생성\n",
    "svm_clf = svm.SVC(random_state = 200)\n",
    "# svm을 이용하여 학습/예측/평가 수행\n",
    "svm_clf.fit(X_train_cnt_vect, y_train)\n",
    "svm_pred = svm_clf.predict(X_test_cnt_vect)\n",
    "print('svm의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,svm_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier의 예측 정확도는 0.175\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier 객체 생성\n",
    "knn_clf = KNeighborsClassifier(n_neighbors = 30)\n",
    "# KNeighborsClassifier을 이용하여 학습/예측/평가 수행\n",
    "knn_clf.fit(X_train_cnt_vect, y_train)\n",
    "knn_pred = knn_clf.predict(X_test_cnt_vect)\n",
    "print('KNeighborsClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,knn_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier의 예측 정확도는 0.601\n"
     ]
    }
   ],
   "source": [
    "# GradientBoostingClassifier 객체 생성\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "# GradientBoostingClassifier을 이용하여 학습/예측/평가 수행\n",
    "gb_clf.fit(X_train_cnt_vect, y_train)\n",
    "gb_pred = gb_clf.predict(X_test_cnt_vect)\n",
    "print('GradientBoostingClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,gb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier의 예측 정확도는 0.420\n",
      "RandomForestClassifier의 예측 정확도는 0.612\n",
      "svm의 예측 정확도는 0.097\n",
      "KNeighborsClassifier의 예측 정확도는 0.175\n",
      "GradientBoostingClassifier의 예측 정확도는 0.601\n"
     ]
    }
   ],
   "source": [
    "print('DecisionTreeClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,dt_pred)))\n",
    "print('RandomForestClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,rf_pred)))\n",
    "print('svm의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,svm_pred)))\n",
    "print('KNeighborsClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,knn_pred)))\n",
    "print('GradientBoostingClassifier의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,gb_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
